{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "DMtH8uno15ng",
    "outputId": "4fdded1b-69ed-4cae-86c7-a8466520ca81"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CskxePzfTOa"
   },
   "outputs": [],
   "source": [
    "data_dir = \"./drive/My Drive/Transformer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4IblGWnFTfI"
   },
   "outputs": [],
   "source": [
    "ko_lines = []\n",
    "with open (data_dir+'korean-english-park.train.ko', mode='r', encoding='utf-8') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        ko_lines.append(line)\n",
    "        if not line: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "9TKiiiNRF0qb",
    "outputId": "7f0b1439-1d4e-4b1b-9ece-a52223009120"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\\n',\n",
       " '모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.\\n',\n",
       " '그러나 이것은 또한 책상도 필요로 하지 않는다.\\n',\n",
       " '79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분이든 그 움직임에따라 커서의 움직임을 조절하는 회전 운동 센서를 사용하고 있다.\\n',\n",
       " '정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔음을 밝혔으며, 세계 해상 교역량의 거의 3분의 1을 운송하는 좁은 해로인 말라카 해협이 테러 공격을 당하기 쉽다고 경고하고 있다.\\n']"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "CS73BxCCF2Um",
    "outputId": "9bb437c6-f3b4-4475-90e8-75a6477a699f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Much of personal computing is about \"can you top this?\"\\n',\n",
       " 'so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.\\n',\n",
       " \"Like all optical mice, But it also doesn't need a desk.\\n\",\n",
       " 'uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.\\n',\n",
       " \"Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.\\n\"]"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lines = []\n",
    "with open (data_dir+\"korean-english-park.train.en\",  mode='r', encoding='utf-8') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        en_lines.append(line)\n",
    "        if not line: break\n",
    "\n",
    "en_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrZy_ZakGP1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from torchtext import data as ttd\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "from torchtext import data as ttd\n",
    "from torchtext.data import Example, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IObHkJg6bgOy"
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ppx0Ka53btIM"
   },
   "outputs": [],
   "source": [
    " sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBRu3NVgGtV9"
   },
   "outputs": [],
   "source": [
    "def build_tokenizer(train_data_path = 'pickles/train_data.pickle'):\n",
    "    \"\"\"\n",
    "    Train soynlp tokenizer which will be used to tokenize Korean input sentence\n",
    "    \"\"\"\n",
    "    print(f'Now building soy-nlp tokenizer . . .')\n",
    "\n",
    "    with open(data_dir+train_data_path, 'rb') as f:\n",
    "      train_data = pickle.load(f)\n",
    "    ko_lines = (train_data.head()['Kor'].tolist())\n",
    "\n",
    "\n",
    "    word_extractor = WordExtractor(min_frequency=5)\n",
    "    word_extractor.train(ko_lines)\n",
    "\n",
    "    word_scores = word_extractor.extract()\n",
    "    cohesion_scores = {word: score.cohesion_forward\n",
    "                       for word, score in word_scores.items()}\n",
    "\n",
    "    with open(data_dir+'pickles/tokenizer.pickle', 'wb') as pickle_out:\n",
    "        pickle.dump(cohesion_scores, pickle_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "8hgZpKGfHE5P",
    "outputId": "1dafc7bc-7234-404d-8113-cb79a2b5c001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now building soy-nlp tokenizer . . .\n",
      "training was done. used memory 1.370 Gb\n",
      "all cohesion probabilities was computed. # words = 100264\n",
      "all branching entropies was computed # words = 192037\n",
      "all accessor variety was computed # words = 192037\n"
     ]
    }
   ],
   "source": [
    "#build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "YZdpNIRV10xk",
    "outputId": "cfabe49f-4240-457e-a23b-e91f45aea3e8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def en_tokenize(text):\n",
    "    text = text.replace('\\\\', ' ')\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jWLIx5oJsKj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`…》]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJpzoy4eJxjR"
   },
   "outputs": [],
   "source": [
    "def convert_to_dataset(data, kor, eng):\n",
    "\n",
    "    # convert each row of DataFrame to torchtext 'Example' containing 'kor' and 'eng' Fields\n",
    "    list_of_examples = [Example.fromlist(row.apply(lambda x: clean_text(x)).tolist(),\n",
    "                                         fields=[('kor', kor), ('eng', eng)]) for _, row in data.iterrows()]\n",
    "\n",
    "    # construct torchtext 'Dataset' using torchtext 'Example' list\n",
    "    dataset = Dataset(examples=list_of_examples, fields=[('kor', kor), ('eng', eng)])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L6dWTLVlL25e",
    "outputId": "1390c4c3-bbc7-44e2-8acb-b66ea9e1d544"
   },
   "outputs": [],
   "source": [
    "pickle_tokenizer = open(data_dir+'pickles/tokenizer.pickle', 'rb')\n",
    "cohesion_scores = pickle.load(pickle_tokenizer)\n",
    "tokenizer = LTokenizer(scores=cohesion_scores)\n",
    "\n",
    "\n",
    "with open(data_dir+'pickles/train_data.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "kor = ttd.Field(tokenize=tokenizer.tokenize,\n",
    "                    lower=True,\n",
    "                    batch_first=True)\n",
    "\n",
    "eng = ttd.Field(tokenize=en_tokenize,\n",
    "                init_token='<sos>',\n",
    "                eos_token='<eos>',\n",
    "                lower=True,\n",
    "                batch_first=True)\n",
    "train_data = convert_to_dataset(train_data, kor, eng)\n",
    "\n",
    "\n",
    "#print(train_data[-1].__dict__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RppDa6dBH7HS"
   },
   "outputs": [],
   "source": [
    "def build_vocab(config):\n",
    "    \"\"\"\n",
    "    Build vocab used to convert input sentence into word indices using soynlp and spacy tokenizer\n",
    "    Args:\n",
    "        config: configuration containing various options\n",
    "    \"\"\"\n",
    "    pickle_tokenizer = open(data_dir+'pickles/tokenizer.pickle', 'rb')\n",
    "    cohesion_scores = pickle.load(pickle_tokenizer)\n",
    "    tokenizer = LTokenizer(scores=cohesion_scores)\n",
    "\n",
    "    # include lengths of the source sentences to use pack pad sequence\n",
    "    kor = ttd.Field(tokenize=tokenizer.tokenize,\n",
    "                    lower=True,\n",
    "                    batch_first=True)\n",
    "\n",
    "    eng = ttd.Field(tokenize=en_tokenize,\n",
    "                    init_token='<sos>',\n",
    "                    eos_token='<eos>',\n",
    "                    lower=True,\n",
    "                    batch_first=True)\n",
    "\n",
    "    with open(data_dir+'pickles/train_data.pickle', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    train_data = convert_to_dataset(train_data, kor, eng)\n",
    "\n",
    "    print(f'Build vocabulary using torchtext . . .')\n",
    "\n",
    "    kor.build_vocab(train_data, max_size=config.kor_vocab)\n",
    "    eng.build_vocab(train_data, max_size=config.eng_vocab)\n",
    "\n",
    "    print(f'Unique tokens in Korean vocabulary: {len(kor.vocab)}')\n",
    "    print(f'Unique tokens in English vocabulary: {len(eng.vocab)}')\n",
    "\n",
    "    print(f'Most commonly used Korean words are as follows:')\n",
    "    print(kor.vocab.freqs.most_common(20))\n",
    "\n",
    "    print(f'Most commonly used English words are as follows:')\n",
    "    print(eng.vocab.freqs.most_common(20))\n",
    "\n",
    "    with open(data_dir+'pickles/kor.pickle', 'wb') as kor_file:\n",
    "        pickle.dump(kor, kor_file)\n",
    "\n",
    "    with open(data_dir+'pickles/eng.pickle', 'wb') as eng_file:\n",
    "        pickle.dump(eng, eng_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "id": "9k4DR58lqCf-",
    "outputId": "45a75aa1-fa38-4199-935c-e0d0dc2bf953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build vocabulary using torchtext . . .\n",
      "Unique tokens in Korean vocabulary: 55002\n",
      "Unique tokens in English vocabulary: 30004\n",
      "Most commonly used Korean words are as follows:\n",
      "[('의', 23305), ('이', 21928), ('는', 19702), ('에', 15945), ('을', 13559), ('가', 13317), ('를', 13315), ('있다', 12952), ('은', 12847), ('고', 11904), ('한', 9071), ('밝혔다', 7939), ('미국', 7718), ('말했다', 7135), ('있는', 6583), ('수', 6356), ('과', 6156), ('로', 6018), ('에서', 5909), ('와', 5712)]\n",
      "Most commonly used English words are as follows:\n",
      "[('the', 127922), ('to', 54251), ('of', 51374), ('a', 49154), ('in', 47100), ('and', 43481), (\"'s\", 21330), ('said', 19622), ('for', 17999), ('that', 17835), ('on', 17830), ('is', 14780), ('was', 13466), ('with', 12430), ('it', 11917), ('as', 10582), ('at', 10573), ('he', 10344), ('by', 9679), ('from', 9502)]\n"
     ]
    }
   ],
   "source": [
    "#build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "TaOD_dun0cga",
    "outputId": "d2e7b845-e185-4aed-cde9-03625100884f"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Pickle Builder')\n",
    "    #parser.add_argument('--train_data_path', type=str, default='pickles/train_data.pickle')\n",
    "    parser.add_argument('--kor_vocab', type=int, default=55000)\n",
    "    parser.add_argument('--eng_vocab', type=int, default=30000)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    build_tokenizer()\n",
    "    build_vocab(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1ASgEroJcT6"
   },
   "outputs": [],
   "source": [
    "# with open(data_dir+'pickles/kor.pickle', 'rb') as kor_file:\n",
    "#       kor = pickle.load(kor_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xd7G3x6zJmYB",
    "outputId": "593c5ae9-ec46-4ad3-d7ad-68ea9fcf3332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "의\n",
      "이\n",
      "는\n",
      "에\n",
      "을\n",
      "가\n",
      "를\n",
      "있다\n",
      "은\n",
      "고\n",
      "한\n",
      "밝혔다\n",
      "미국\n",
      "말했다\n",
      "있는\n",
      "수\n",
      "과\n",
      "로\n",
      "에서\n",
      "와\n",
      "대통령\n",
      "할\n",
      "지난\n",
      "그는\n",
      "200\n",
      "위해\n",
      "대한\n",
      "인\n",
      "정부\n",
      "이번\n",
      "전했다\n",
      "것이\n",
      "대해\n",
      "며\n",
      "것이라고\n",
      "하는\n",
      "된\n",
      "전\n",
      "다\n",
      "이라크\n",
      "해\n",
      "했다\n",
      "것으로\n",
      "더\n",
      "도\n",
      "것을\n",
      "현지시간\n",
      "중국\n",
      "북한\n",
      "”고\n",
      "한국\n",
      "중\n",
      "하고\n",
      "될\n",
      "다른\n",
      "그러나\n",
      "으로\n",
      "미\n",
      "영국\n",
      "문제\n",
      "오바마\n",
      "10\n",
      "경찰\n",
      "총리\n",
      "또\n",
      "세계\n",
      "한편\n",
      "에게\n",
      "영화\n",
      "위한\n",
      "후보\n",
      "관련\n",
      "동안\n",
      "여성\n",
      "많은\n",
      "주\n",
      "통해\n",
      "이후\n",
      "일본\n",
      "가장\n",
      "같은\n",
      "지\n",
      "부시\n",
      "그의\n",
      "후\n",
      "그\n",
      "서\n",
      "우리\n",
      "않았다\n",
      "국가\n",
      "대변인은\n",
      "한다\n",
      "됐다\n",
      "테러\n",
      "20\n",
      "였다\n",
      "”며\n",
      "계획\n"
     ]
    }
   ],
   "source": [
    "# for i in range(100):\n",
    "#   print(kor.vocab.itos[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
